{
 "cells": [
  {
   "cell_type": "raw",
   "id": "05393811",
   "metadata": {},
   "source": [
    "# Manifold Markets tool for user statistics\n",
    "# NOTE, JSON INCLUDES DELETED USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1a40feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Update User Database from API #\n",
    "#################################\n",
    "\n",
    "####### CONFIG ##########\n",
    "# Manifold Supabase API key\n",
    "api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB4aWRyZ2thdHVtbHZmcWF4Y2xsIiwicm9sZSI6ImFub24iLCJpYXQiOjE2Njg5OTUzOTgsImV4cCI6MTk4NDU3MTM5OH0.d_yYtASLzAoIIGdXUBIgRAGLBnNow7JG2SoaNMQ8ySg\"\n",
    "#########################\n",
    "\n",
    "supabase_rest_api_url = \"https://pxidrgkatumlvfqaxcll.supabase.co/rest/v1/\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "# timezone used for DAU in manifold (if the measurement is indeed done at midnight) (this is UTC-08:00)\n",
    "timezone_pacific = pytz.timezone('Pacific/Pitcairn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd271759",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pxidrgkatumlvfqaxcll.supabase.co/rest/v1/contracts?select=*&id=eq.2sWuaQx9Y66eWHGadh8B&limit=1\n"
     ]
    }
   ],
   "source": [
    "# API request headers\n",
    "\n",
    "headers = {\n",
    "    \"Apikey\": f\"{api_key}\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "}\n",
    "userid = 'APtmK3Sep3TJB92vIgWeSmAEwVa2'\n",
    "#table = 'user_contract_metrics'\n",
    "#query = 'select=*&user_id=eq.' + userid + '&limit=10'\n",
    "table = 'contracts'\n",
    "#query = 'select=*&contracts_visibility_public=eq.2sWuaQx9Y66eWHGadh8B&limit=1'\n",
    "query = 'select=*&id=eq.2sWuaQx9Y66eWHGadh8B&limit=1'\n",
    "url = supabase_rest_api_url + table + '?' + query\n",
    "print(url)\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "#https://pxidrgkatumlvfqaxcll.supabase.co/rest/v1/user_contract_metrics?select=*&user_id=eq.APtmK3Sep3TJB92vIgWeSmAEwVa2&limit=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57dc0a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[{\"id\":\"2sWuaQx9Y66eWHGadh8B\",\"data\":{\"p\": 0.47351816818703873, \"id\": \"2sWuaQx9Y66eWHGadh8B\", \"pool\": {\"NO\": 530.110044586846, \"YES\": 1207.4106615313506}, \"prob\": 0.28309205377859303, \"slug\": \"will-a-hurricane-make-landfall-in-t-d9549c66acae\", \"volume\": 8032.408241373813, \"question\": \"Will a hurricane make landfall in Texas at any point during 2023?\", \"closeTime\": 1701373620000, \"creatorId\": \"4JuXgDx47xPagH5mcLDqLzUSN5g2\", \"mechanism\": \"cpmm-1\", \"dailyScore\": 2.657732970556088, \"elasticity\": 0.5160849576002623, \"groupLinks\": [{\"name\": \"Weather\", \"slug\": \"weather\", \"groupId\": \"Vw7PuCkPJlK51z43wA4k\", \"createdTime\": 1691777960009}, {\"name\": \"2023 Hurricane Season\", \"slug\": \"2023-hurricane-season\", \"groupId\": \"QsUuBJ6NtclsnGucs4OG\", \"createdTime\": 1691777960007}, {\"name\": \"Climate\", \"slug\": \"climate\", \"groupId\": \"97oNExy8iFftY2EgdkLw\", \"createdTime\": 1691777960140}], \"groupSlugs\": [\"weather\", \"2023-hurricane-season\", \"climate\"], \"isResolved\": false, \"visibility\": \"public\", \"createdTime\": 1691777959892, \"creatorName\": \"Brian T. Edwards\", \"description\": {\"type\": \"doc\", \"content\": [{\"type\": \"paragraph\", \"content\": [{\"text\": \"Could happen after the end of official hurricane season and still count. \", \"type\": \"text\"}]}]}, \"lastBetTime\": 1693812362744, \"outcomeType\": \"BINARY\", \"probChanges\": {\"day\": -0.18405899771138112, \"week\": -0.2665582449589785, \"month\": -0.21690794622140697}, \"subsidyPool\": 0, \"collectedFees\": {\"creatorFee\": 0, \"platformFee\": 0, \"liquidityFee\": 0}, \"coverImageUrl\": \"https://firebasestorage.googleapis.com/v0/b/mantic-markets.appspot.com/o/dream%2F9FvG05ROD_.png?alt=media&token=e112d8c1-d2fb-4152-9a19-503705bf5c7d\", \"nonPredictive\": false, \"volume24Hours\": 710, \"totalLiquidity\": 810, \"closeEmailsSent\": 1, \"creatorUsername\": \"BTE\", \"importanceScore\": 0.48943505028383366, \"lastCommentTime\": 1693780390269, \"lastUpdatedTime\": 1693812363079, \"popularityScore\": 13.7, \"creatorAvatarUrl\": \"https://firebasestorage.googleapis.com/v0/b/mantic-markets.appspot.com/o/user-images%2FBTE%2FaSsPoXaRJr.png?alt=media&token=bb7d2e7c-7345-43a6-aae9-3fcd8f17498c\", \"likedByUserCount\": 1, \"uniqueBettorCount\": 42, \"creatorCreatedTime\": 1645484592545, \"initialProbability\": 0.5},\"fs_updated_time\":\"2023-09-04T11:05:44.736\",\"slug\":\"will-a-hurricane-make-landfall-in-t-d9549c66acae\",\"question\":\"Will a hurricane make landfall in Texas at any point during 2023?\",\"creator_id\":\"4JuXgDx47xPagH5mcLDqLzUSN5g2\",\"visibility\":\"public\",\"mechanism\":\"cpmm-1\",\"outcome_type\":\"BINARY\",\"created_time\":\"2023-08-11T18:19:19.892+00:00\",\"close_time\":\"2023-11-30T19:47:00+00:00\",\"resolution_time\":null,\"resolution_probability\":null,\"resolution\":null,\"popularity_score\":13.7,\"question_fts\":\"'2023':12 'hurrican':3 'landfal':5 'make':4 'point':10 'texa':7\",\"description_fts\":\"'brian':1 'could':4 'count':15 'edward':3 'end':8 'happen':5 'hurrican':11 'offici':10 'season':12 'still':14\",\"question_nostop_fts\":\"'2023':12 'a':2 'ani':9 'at':8 'dure':11 'hurrican':3 'in':6 'landfal':5 'make':4 'point':10 'texa':7 'will':1\",\"importance_score\":0.48943505028383366}]\n",
      "[{'id': '2sWuaQx9Y66eWHGadh8B', 'data': {'p': 0.47351816818703873, 'id': '2sWuaQx9Y66eWHGadh8B', 'pool': {'NO': 530.110044586846, 'YES': 1207.4106615313506}, 'prob': 0.28309205377859303, 'slug': 'will-a-hurricane-make-landfall-in-t-d9549c66acae', 'volume': 8032.408241373813, 'question': 'Will a hurricane make landfall in Texas at any point during 2023?', 'closeTime': 1701373620000, 'creatorId': '4JuXgDx47xPagH5mcLDqLzUSN5g2', 'mechanism': 'cpmm-1', 'dailyScore': 2.657732970556088, 'elasticity': 0.5160849576002623, 'groupLinks': [{'name': 'Weather', 'slug': 'weather', 'groupId': 'Vw7PuCkPJlK51z43wA4k', 'createdTime': 1691777960009}, {'name': '2023 Hurricane Season', 'slug': '2023-hurricane-season', 'groupId': 'QsUuBJ6NtclsnGucs4OG', 'createdTime': 1691777960007}, {'name': 'Climate', 'slug': 'climate', 'groupId': '97oNExy8iFftY2EgdkLw', 'createdTime': 1691777960140}], 'groupSlugs': ['weather', '2023-hurricane-season', 'climate'], 'isResolved': False, 'visibility': 'public', 'createdTime': 1691777959892, 'creatorName': 'Brian T. Edwards', 'description': {'type': 'doc', 'content': [{'type': 'paragraph', 'content': [{'text': 'Could happen after the end of official hurricane season and still count. ', 'type': 'text'}]}]}, 'lastBetTime': 1693812362744, 'outcomeType': 'BINARY', 'probChanges': {'day': -0.18405899771138112, 'week': -0.2665582449589785, 'month': -0.21690794622140697}, 'subsidyPool': 0, 'collectedFees': {'creatorFee': 0, 'platformFee': 0, 'liquidityFee': 0}, 'coverImageUrl': 'https://firebasestorage.googleapis.com/v0/b/mantic-markets.appspot.com/o/dream%2F9FvG05ROD_.png?alt=media&token=e112d8c1-d2fb-4152-9a19-503705bf5c7d', 'nonPredictive': False, 'volume24Hours': 710, 'totalLiquidity': 810, 'closeEmailsSent': 1, 'creatorUsername': 'BTE', 'importanceScore': 0.48943505028383366, 'lastCommentTime': 1693780390269, 'lastUpdatedTime': 1693812363079, 'popularityScore': 13.7, 'creatorAvatarUrl': 'https://firebasestorage.googleapis.com/v0/b/mantic-markets.appspot.com/o/user-images%2FBTE%2FaSsPoXaRJr.png?alt=media&token=bb7d2e7c-7345-43a6-aae9-3fcd8f17498c', 'likedByUserCount': 1, 'uniqueBettorCount': 42, 'creatorCreatedTime': 1645484592545, 'initialProbability': 0.5}, 'fs_updated_time': '2023-09-04T11:05:44.736', 'slug': 'will-a-hurricane-make-landfall-in-t-d9549c66acae', 'question': 'Will a hurricane make landfall in Texas at any point during 2023?', 'creator_id': '4JuXgDx47xPagH5mcLDqLzUSN5g2', 'visibility': 'public', 'mechanism': 'cpmm-1', 'outcome_type': 'BINARY', 'created_time': '2023-08-11T18:19:19.892+00:00', 'close_time': '2023-11-30T19:47:00+00:00', 'resolution_time': None, 'resolution_probability': None, 'resolution': None, 'popularity_score': 13.7, 'question_fts': \"'2023':12 'hurrican':3 'landfal':5 'make':4 'point':10 'texa':7\", 'description_fts': \"'brian':1 'could':4 'count':15 'edward':3 'end':8 'happen':5 'hurrican':11 'offici':10 'season':12 'still':14\", 'question_nostop_fts': \"'2023':12 'a':2 'ani':9 'at':8 'dure':11 'hurrican':3 'in':6 'landfal':5 'make':4 'point':10 'texa':7 'will':1\", 'importance_score': 0.48943505028383366}]\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print(response.text)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c025e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-14 01:54:42-08\n",
      "2023-08-14 02:11:22-08\n"
     ]
    }
   ],
   "source": [
    "ts_milli_start = 1692006882400\n",
    "ts_milli_end = 1692007882400\n",
    "contract_date = datetime.fromtimestamp(ts_milli_start / 1000, tz=timezone_pacific)\n",
    "#start_ts =  datetime.fromtimestamp(1692006882400 / 1000, tz=timezone_pacific)\n",
    "#end_ts =  datetime.fromtimestamp(1692006882600 / 1000, tz=timezone_pacific)\n",
    "\n",
    "\n",
    "# Convert milliseconds to seconds\n",
    "start_timestamp_seconds = ts_milli_start / 1000\n",
    "end_timestamp_seconds = ts_milli_end / 1000\n",
    "\n",
    "# Create datetime objects in Pacific Time\n",
    "start_datetime_pacific = datetime.fromtimestamp(start_timestamp_seconds, tz=timezone_pacific)\n",
    "end_datetime_pacific = datetime.fromtimestamp(end_timestamp_seconds, tz=timezone_pacific)\n",
    "\n",
    "# Format datetime objects to the required TIMESTAMP WITH TIME ZONE format\n",
    "timestamp_format = '%Y-%m-%d %H:%M:%S%z'\n",
    "start_formatted = start_datetime_pacific.strftime(timestamp_format)\n",
    "end_formatted = end_datetime_pacific.strftime(timestamp_format)\n",
    "\n",
    "# Remove the minutes from the timezone\n",
    "start_formatted = start_formatted[:-2]\n",
    "end_formatted = end_formatted[:-2]\n",
    "\n",
    "# URL-encode the plus sign (+) in the timestamps\n",
    "start_formatted_urlencoded = start_formatted.replace('+', '%2B')\n",
    "end_formatted_urlencoded = end_formatted.replace('+', '%2B')\n",
    "\n",
    "print(start_formatted_urlencoded)\n",
    "print(end_formatted_urlencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa9c0b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pxidrgkatumlvfqaxcll.supabase.co/rest/v1/contracts?select=data->id,data->creatorId,data->createdTime&created_time=gt.2023-08-14 01:54:42-08&created_time=lt.2023-08-14 02:11:22-08&limit=10\n"
     ]
    }
   ],
   "source": [
    "#query = 'select extract(day from (millis_to_ts(' + str(ts_milli) + ') - created_time)) as day, created_time, creator_id, id from contracts&user_id=eq.APtmK3Sep3TJB92vIgWeSmA'\n",
    "#    where created_time >= millis_to_ts($1) and created_time < millis_to_ts($2)`\n",
    "\n",
    "table = table = 'contracts'\n",
    "#query = f\"select=data:created_time,data:creator_id,data:id&created_time=gt.{start_formatted_urlencoded}&created_time=lt.{end_formatted_urlencoded}&limit=10\"\n",
    "query = f\"select=data->id,data->creatorId,data->createdTime&created_time=gt.{start_formatted_urlencoded}&created_time=lt.{end_formatted_urlencoded}&limit=10\"\n",
    "#query = f\"select=data->createdTime&created_time=gt.{start_formatted_urlencoded}&created_time=lt.{end_formatted_urlencoded}&limit=10\"\n",
    "#query = 'select=created_time,creator_id,id&day=extract.day,millis_to_ts.gt.eq.' + str(ts_milli_start) + '&created_time=gt.' + str(ts_milli_start) + '&created_time=lt.' + str(ts_milli_end) + '&user_id=eq.APtmK3Sep3TJB92vIgWeSmAEwVa2&limit=10'\n",
    "url = supabase_rest_api_url + table + '?' + query\n",
    "print(url)\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45a1b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pxidrgkatumlvfqaxcll.supabase.co/rest/v1/contracts?select=data->id,data->creatorId,data->createdTime&created_time=gt.2023-08-14 01:54:42-08&created_time=lt.2023-08-14 02:11:22-08&limit=10\n",
      "\n",
      "[{'id': 'YVrftn67acp2py2GAoCy', 'creatorId': 'd3nWMAHzhvZS2d8qsqJts5irDuj2', 'createdTime': 1692007420887}, {'id': 'ZBheRfS9B8LYE9XPaPFm', 'creatorId': '4xOTMCIOkGesdJft50wVFZFb5IB3', 'createdTime': 1692007389961}, {'id': 'YPhenv5Mbu89zqQ2CQwV', 'creatorId': '4xOTMCIOkGesdJft50wVFZFb5IB3', 'createdTime': 1692007235487}]\n",
      "\n",
      "2023-08-14 02:03:40.887000-08:00\n",
      "2023-08-14 02:03:09.961000-08:00\n",
      "2023-08-14 02:00:35.487000-08:00\n"
     ]
    }
   ],
   "source": [
    "print(url)\n",
    "print(\"\")\n",
    "\n",
    "print(data)\n",
    "print(\"\")\n",
    "for contract in data:\n",
    "    mstime = contract['createdTime']\n",
    "    contract_date = datetime.fromtimestamp(mstime / 1000, tz=timezone_pacific)\n",
    "    print(contract_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4d26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "db16e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use absolute path so we can use this in other notebooks\n",
    "all_users_filepath = \"/home/db/Documents/JRPdata/manifold-users/all_users.json\"\n",
    "\n",
    "limit_url = \"https://manifold.markets/api/v0/users?limit=1000\"\n",
    "limit_before_url = \"https://manifold.markets/api/v0/users?limit=1000&before=\"\n",
    "limit_before_url_single = \"https://manifold.markets/api/v0/users?limit=1&before=\"\n",
    "\n",
    "# API request headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Key {api_key}\",\n",
    "}\n",
    "\n",
    "# API request to get all users\n",
    "#   For when there is no data stored locally (first run), or we want to update the info for every user\n",
    "def get_all_users():\n",
    "    all_users = {}\n",
    "    # for first run\n",
    "    url = limit_url\n",
    "    total_count = 0\n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            num_users = len(data)\n",
    "            if isinstance(data, list) and num_users > 0:\n",
    "                for user_data in data:\n",
    "                    if isinstance(user_data, dict) and \"id\" in user_data:\n",
    "                        user_id = user_data[\"id\"]\n",
    "                        all_users[user_id] = user_data  # Use the user_id as the key in all_users\n",
    "                total_count += num_users\n",
    "                print(f\" -- API - downloaded data for {total_count} users...\")\n",
    "                time.sleep(10)  # Delay between requests to avoid rate limits\n",
    "                # assume the last user_id is the oldest one in the JSON\n",
    "                url = limit_before_url + user_id\n",
    "            else:\n",
    "                if isinstance(data, dict) and 'error' in data:\n",
    "                    print(\"Error, data from API not a list or no data: {data}\")\n",
    "                url = None\n",
    "            if num_users < 1000:\n",
    "                # requested a 1000, so if there is less we have hit the end of the users\n",
    "                url = None\n",
    "        else:\n",
    "            print(\"Error from API:\", response.json()[\"error\"])\n",
    "            return None\n",
    "    return all_users\n",
    "\n",
    "# this is to only update for the new users (don't include data for old user data to limit API requests)\n",
    "def get_users_after(after_user_id):\n",
    "    users_after = {}\n",
    "    # for first run\n",
    "    url = limit_url\n",
    "    total_count = 0\n",
    "    if after_user_id is None:\n",
    "        print(\"Error: get_users_after() must have non-empty user_id\")\n",
    "        return None\n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            num_users = len(data)\n",
    "            if isinstance(data, list) and num_users > 0:\n",
    "                for user_data in data:\n",
    "                    if isinstance(user_data, dict) and \"id\" in user_data:\n",
    "                        user_id = user_data[\"id\"]\n",
    "                        if user_id == after_user_id:\n",
    "                            print(f\" -- API - data for {total_count} new users...\")\n",
    "                            return users_after\n",
    "                        total_count += 1\n",
    "                        users_after[user_id] = user_data  # Use the user_id as the key in users_after\n",
    "                time.sleep(10)  # Delay between requests to avoid rate limits\n",
    "                # assume the last user_id is the oldest one in the JSON\n",
    "                url = limit_before_url + user_id\n",
    "            else:\n",
    "                if isinstance(data, dict) and 'error' in data:\n",
    "                    print(\"Error, data from API not a list or no data: {data}\")\n",
    "                url = None\n",
    "            if num_users < 1000:\n",
    "                # requested a 1000, so if there is less we have hit the end of the users\n",
    "                url = None\n",
    "        else:\n",
    "            print(\"Error from API:\", response.json()[\"error\"])\n",
    "            return None\n",
    "    return users_after\n",
    "\n",
    "def get_newest_user_id(all_users):\n",
    "    if not all_users:\n",
    "        return None\n",
    "\n",
    "    max_created_time = float('-inf')\n",
    "    user_with_highest_created_time = None\n",
    "\n",
    "    for user_id, user_data in all_users.items():\n",
    "        created_time = user_data.get(\"createdTime\", 0)\n",
    "        if created_time > max_created_time:\n",
    "            max_created_time = created_time\n",
    "            user_with_highest_created_time = user_id\n",
    "    return user_with_highest_created_time\n",
    "\n",
    "def update_for_new_users(all_users_filepath):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(all_users_filepath):\n",
    "        all_users = get_all_users()\n",
    "        # Check if all_users is not an empty dict and not None\n",
    "        if all_users is not None and isinstance(all_users, dict) and all_users:\n",
    "            # Write the json to the file at all_users_filepath\n",
    "            with open(all_users_filepath, \"w\") as file:\n",
    "                json.dump(all_users, file)\n",
    "            num_users = len(all_users)\n",
    "            print(f\"Updated (all) users. Total users found: {num_users}\")\n",
    "        else:\n",
    "            print(\"Error: list of users retreived from API is empty or None\")\n",
    "    else:\n",
    "        try:\n",
    "            # Read the existing data from the file (assuming it contains an dict)\n",
    "            with open(all_users_filepath, \"r\") as file:\n",
    "                existing_data = json.load(file)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle the case when the file contains invalid JSON data\n",
    "            print(f\"Error: Invalid JSON data in {all_users_filepath}. Not updating.\")\n",
    "            return\n",
    "        # Do a minimal update by getting previous newest user ID\n",
    "        prev_newest_user_id = get_newest_user_id(existing_data)\n",
    "        if not prev_newest_user_id:\n",
    "            print(\"Error: Could not do minimal update with no existing newest user_id found.\")\n",
    "            return\n",
    "        \n",
    "        users_after = get_users_after(prev_newest_user_id)\n",
    "        new_num_users = len(users_after)\n",
    "        # put the old data after users_after\n",
    "        users_after.update(existing_data)\n",
    "        with open(all_users_filepath, \"w\") as file:\n",
    "            json.dump(users_after, file)\n",
    "        total_num_users = len(users_after)\n",
    "        print(f\"Updated (all) users. New users: {new_num_users}. Total users found: {total_num_users}\")\n",
    "    print(\"Done updating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "abc75668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- API - data for 549 new users...\n",
      "Updated (all) users. New users: 549. Total users found: 37175\n",
      "Done updating.\n"
     ]
    }
   ],
   "source": [
    "# do the update\n",
    "#print(\"Updating for new users...\")\n",
    "#update_for_new_users(all_users_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bc2db2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph user statistics\n",
    "import pytz\n",
    "# timezone used for DAU in manifold (if the measurement is indeed done at midnight) (this is UTC-08:00)\n",
    "timezone_pacific = pytz.timezone('Pacific/Pitcairn')\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to filter user_data within a date range\n",
    "def filter_user_data(all_users, start_date, end_date):\n",
    "    filtered_data = {}\n",
    "    for user_id in all_users:\n",
    "        if start_date <= all_users[user_id]['createdTime'] <= end_date:\n",
    "            filtered_data[user_id] = all_users[user_id]\n",
    "    return filtered_data\n",
    "\n",
    "# Function to plot the filtered user_data\n",
    "def plot_user_data(all_users_filepath=all_users_filepath, bin_width='Daily', days_ago=3):\n",
    "    print(\"Plotting\")\n",
    "    local_timezone = datetime.now().astimezone().tzinfo\n",
    "    now = datetime.now(local_timezone)\n",
    "    start_date = now - timedelta(days=days_ago)\n",
    "    # Load user_data (example data in JSON format)\n",
    "    with open(all_users_filepath, \"r\") as file:\n",
    "        all_users = json.load(file)\n",
    "    # Convert timestamps to datetime objects\n",
    "    for user_id in all_users:\n",
    "        all_users[user_id]['createdTime'] = datetime.fromtimestamp(all_users[user_id]['createdTime'] / 1000, tz=timezone_pacific)\n",
    "    filtered_data = filter_user_data(all_users, start_date, now)\n",
    "    \n",
    "    print(len(all_users))\n",
    "    print(len(filtered_data))\n",
    "    if bin_width == 'Daily':\n",
    "        bin_width = 1\n",
    "        x_label = 'Date (Daily)'\n",
    "    elif bin_width == 'Hourly':\n",
    "        bin_width = 1 / 24\n",
    "        x_label = 'Date (Hourly)'\n",
    "    else:\n",
    "        bin_width = 1 / (24 * 60)\n",
    "        x_label = 'Date (Minutes)'\n",
    "\n",
    "    timestamps = []\n",
    "    # Convert datetime objects to timestamps (floats) for binning\n",
    "    for user_id, user_data in filtered_data.items():\n",
    "        float_timestamp = user_data['createdTime'].timestamp()\n",
    "        timestamps.append(float_timestamp)\n",
    "        print(float_timestamp)\n",
    "    \n",
    "    # Create bins and corresponding counts\n",
    "    num_bins = int((now - start_date).total_seconds() / (bin_width * 60))\n",
    "    x_bins = [start_date + timedelta(minutes=i * bin_width * 60) for i in range(num_bins)]\n",
    "    counts, _ = np.histogram(timestamps, bins=num_bins)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x_bins, counts, width=timedelta(minutes=bin_width * 60), align='center')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Number of Signups')\n",
    "    plt.title('Recent Signups')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Create interactive GUI elements\n",
    "binning_dropdown = widgets.Dropdown(\n",
    "    options=['Daily', 'Hourly', 'Minutes'],\n",
    "    value='Daily',\n",
    "    description='Binning:',\n",
    "    layout=Layout(width='1920px')\n",
    ")\n",
    "\n",
    "# Set up the interactivity\n",
    "# interact(plot_user_data, all_users_filepath=all_users_filepath, bin_width=binning_dropdown, days_ago=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e8978e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New user statistics\n",
    "import pytz\n",
    "# timezone used for DAU in manifold (if the measurement is indeed done at midnight) (this is UTC-08:00)\n",
    "timezone_pacific = pytz.timezone('Pacific/Pitcairn')\n",
    "\n",
    "def count_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath):\n",
    "    # Check if max_timestamp is \"Now\" (case-insensitive) and replace it with the current timestamp\n",
    "    if max_timestamp.lower() == \"now\":\n",
    "        local_timezone = datetime.now().astimezone().tzinfo\n",
    "        max_timestamp = datetime.now(local_timezone).isoformat()\n",
    "\n",
    "    try:\n",
    "        # Read all_users from the file\n",
    "        with open(all_users_filepath, \"r\") as file:\n",
    "            all_users = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {all_users_filepath} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON data in {all_users_filepath}.\")\n",
    "        return\n",
    "\n",
    "    # Convert the timestamp strings to datetime objects\n",
    "    min_time = datetime.fromisoformat(min_timestamp)\n",
    "    max_time = datetime.fromisoformat(max_timestamp)\n",
    "\n",
    "    # Count users whose createdTime is within the given time range\n",
    "    count = 0\n",
    "    for user_id, user_data in all_users.items():\n",
    "        if \"createdTime\" in user_data:\n",
    "            created_time = datetime.fromtimestamp(user_data[\"createdTime\"] / 1000, tz=timezone_pacific)\n",
    "            if min_time <= created_time <= max_time:\n",
    "                count += 1\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of users created between {min_timestamp} and {max_timestamp}: {count}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "25bc6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users created between 2023-06-25T00:00:00-08:00 and 2023-07-25T00:00:00-08:00: 1459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmin_timestamp = \"2023-08-02T00:00:00-08:00\"\\nmax_timestamp = \"2023-08-02T12:00:00-08:00\"\\ncount_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\\nmin_timestamp = \"2023-08-02T12:00:00-08:00\"\\nmax_timestamp = \"2023-08-03T00:00:00-08:00\"\\ncount_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\\n\\nmin_timestamp = \"2023-08-03T00:00:00-08:00\"\\nmax_timestamp = \"2023-08-03T12:00:00-08:00\"\\ncount_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\\nmin_timestamp = \"2023-08-03T12:00:00-08:00\"\\nmax_timestamp = \"2023-08-04T00:00:00-08:00\"\\ncount_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\\n'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of new users between an interval of dates \n",
    "# make sure to adjust the time correctly if you want to use a different timezone or period\n",
    "# the DAU is calculated at 00:00:00-08:00 (midnight Pacific Timezone)\n",
    "min_timestamp = \"2023-06-25T00:00:00-08:00\"\n",
    "#min_timestamp = \"2023-07-30T15:44:14+00:00\"\n",
    "# max_timestamp = \"2023-07-29T08:00:00+00:00\"\n",
    "# Can use \"now\" or a timestamp in the same format as the max_timestamp\n",
    "max_timestamp = \"2023-07-25T00:00:00-08:00\"\n",
    "#max_timestamp = \"now\"\n",
    "count_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\n",
    "'''\n",
    "min_timestamp = \"2023-08-02T00:00:00-08:00\"\n",
    "max_timestamp = \"2023-08-02T12:00:00-08:00\"\n",
    "count_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\n",
    "min_timestamp = \"2023-08-02T12:00:00-08:00\"\n",
    "max_timestamp = \"2023-08-03T00:00:00-08:00\"\n",
    "count_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\n",
    "\n",
    "min_timestamp = \"2023-08-03T00:00:00-08:00\"\n",
    "max_timestamp = \"2023-08-03T12:00:00-08:00\"\n",
    "count_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\n",
    "min_timestamp = \"2023-08-03T12:00:00-08:00\"\n",
    "max_timestamp = \"2023-08-04T00:00:00-08:00\"\n",
    "count_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f9a1f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Iterate over the range of num_days and print the string for each day\\nfor day in range(num_days):\\n    # Add 'day' number of days to the datetime object\\n    updated_datetime = start_datetime + timedelta(days=day)\\n    # Convert the updated datetime object back to the desired string format\\n    updated_start = str(updated_datetime.isoformat() + start[-6:])  # Append the timezone offset back\\n    # do the same for the end time\\n    updated_datetime = start_datetime + timedelta(days=day) + timedelta(days=1)\\n    updated_end = str(updated_datetime.isoformat() + start[-6:])  # Append the timezone offset back\\n    count_users_in_time_range(updated_start, updated_end, all_users_filepath)\\n\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = \"2023-07-01T00:00:00-08:00\"\n",
    "num_days = 40  # Change this value to the desired number of days to look over\n",
    "\n",
    "# Parse the string into a datetime object\n",
    "start_datetime = datetime.fromisoformat(start[:-6])\n",
    "'''\n",
    "# Iterate over the range of num_days and print the string for each day\n",
    "for day in range(num_days):\n",
    "    # Add 'day' number of days to the datetime object\n",
    "    updated_datetime = start_datetime + timedelta(days=day)\n",
    "    # Convert the updated datetime object back to the desired string format\n",
    "    updated_start = str(updated_datetime.isoformat() + start[-6:])  # Append the timezone offset back\n",
    "    # do the same for the end time\n",
    "    updated_datetime = start_datetime + timedelta(days=day) + timedelta(days=1)\n",
    "    updated_end = str(updated_datetime.isoformat() + start[-6:])  # Append the timezone offset back\n",
    "    count_users_in_time_range(updated_start, updated_end, all_users_filepath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f2b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def search_user_bios_by_keyword(keyword_string_list, all_users_filepath):\n",
    "    try:\n",
    "        # Read all_users from the file\n",
    "        with open(all_users_filepath, \"r\") as file:\n",
    "            all_users = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {all_users_filepath} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON data in {all_users_filepath}.\")\n",
    "        return\n",
    "\n",
    "    # Filter users whose bio contains any of the keywords in the keyword_string_list\n",
    "    matched_user_ids = []\n",
    "    num_non_deleted_users = 0\n",
    "    num_users_with_bio = 0\n",
    "    for user_id, user_data in all_users.items():\n",
    "        # skip deleted users\n",
    "        if 'userDeleted' in user_data and user_data['userDeleted']:\n",
    "            continue\n",
    "        num_non_deleted_users += 1\n",
    "        if 'bio' in user_data:\n",
    "            bio = user_data[\"bio\"]\n",
    "            if len(bio) > 3:\n",
    "                num_users_with_bio += 1\n",
    "            for keyword in keyword_string_list:\n",
    "                if keyword.lower() in bio.lower():\n",
    "                    matched_user_ids.append(user_id)\n",
    "                    print('Name: ', user_data['name'])\n",
    "                    print('  User URL:', user_data['url'])\n",
    "                    if 'website' in user_data:\n",
    "                        print('  Website: ', user_data['website'])\n",
    "                    if 'twitterHandle' in user_data:\n",
    "                        print('  Twitter URL: ', 'https://www.twitter.com/', user_data['twitterHandle'])\n",
    "                    if 'discordHandle' in user_data:\n",
    "                        print('  Discord handle: ', user_data['discordHandle'])\n",
    "                    if 'bio' in user_data:\n",
    "                        print('  Bio: ', user_data['bio'])\n",
    "                    break  # Break the inner loop if a match is found for this user\n",
    "    bio_pct = 100 * num_users_with_bio / num_non_deleted_users\n",
    "    print(f'Num users with bio field (> 3 chars) / total (non-deleted) users: {num_users_with_bio} / {num_non_deleted_users} : {bio_pct:5.3}%')\n",
    "    return matched_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00ae090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_string_list = [\"Physics\", \"Physicist\", \"Condensed Matter\", \"Superconduct\"]\n",
    "\n",
    "# matched_users = search_user_bios_by_keyword(keyword_string_list, all_users_filepath)\n",
    "# print(\"Number of users with matching bios: \", len(matched_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005cf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dce0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_username_to_user_id_mapping(all_users_filepath):\n",
    "    # Load all_users data from the JSON file\n",
    "    with open(all_users_filepath, 'r') as file:\n",
    "        all_users_data = json.load(file)\n",
    "\n",
    "    # Create a dictionary with usernames as keys and user IDs as values\n",
    "    username_to_user_id = {user['username']: user_id for user_id, user in all_users_data.items()}\n",
    "\n",
    "    return username_to_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9fd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_user_ids_before_user_id(user_ids_list, all_users_filepath):\n",
    "    # Create a dictionary to store user IDs for accounts created (timewise) before given user_id\n",
    "    user_ids_before_user_id = {}\n",
    "\n",
    "    # This assumes that the JSON list for all_users was created properly, in descending order, as from the API\n",
    "    # Loop through the user_ids in the list\n",
    "    for user_id in user_ids_list:\n",
    "        all_users_data = json.load(open(all_users_filepath, 'r'))\n",
    "        index = list(all_users_data.keys()).index(user_id)\n",
    "        if index > 0:\n",
    "            before_user_id = list(all_users_data.keys())[index - 1]\n",
    "            user_ids_before_user_id[user_id] = before_user_id\n",
    "    return user_ids_before_user_id\n",
    "\n",
    "def find_user_ids_before_usernames(username_list, all_users_filepath):\n",
    "    # Get the mapping of usernames to user IDs using the previous function\n",
    "    username_to_user_id_mapping = get_username_to_user_id_mapping(all_users_filepath)\n",
    "\n",
    "    # Create a dictionary to store user IDs for accounts created (timewise) before given usernames\n",
    "    user_ids_before_usernames = {}\n",
    "\n",
    "    # This assumes that the JSON list for all_users was created properly, in descending order, as from the API\n",
    "    # Loop through the usernames in the list\n",
    "    for username in username_list:\n",
    "        if username in username_to_user_id_mapping:\n",
    "            user_id = username_to_user_id_mapping[username]\n",
    "            all_users_data = json.load(open(all_users_filepath, 'r'))\n",
    "            index = list(all_users_data.keys()).index(user_id)\n",
    "            if index > 0:\n",
    "                before_user_id = list(all_users_data.keys())[index - 1]\n",
    "                user_ids_before_usernames[username] = before_user_id\n",
    "\n",
    "    return user_ids_before_usernames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd8635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the manifold API using data from all_users.json to find the user\n",
    "# Can print data for user\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# api only returns user data for a user that was created (in time) before a given user_id\n",
    "def get_single_user_data_after(user_id_after):\n",
    "    users_after = {}\n",
    "    # for first run\n",
    "    url = limit_before_url_single + user_id_after\n",
    "    total_count = 0\n",
    "    if user_id_after is None:\n",
    "        print(\"Error: get_single_user_data_before() must have non-empty user_id\")\n",
    "        return None\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        num_users = len(data)\n",
    "        if isinstance(data, list) and num_users > 0:\n",
    "            for user_data in data:\n",
    "                if isinstance(user_data, dict) and \"id\" in user_data:\n",
    "                    return user_data\n",
    "            time.sleep(1)  # Delay between requests to avoid rate limits\n",
    "        else:\n",
    "            print(\"Error, did not get expected data from API: {data}\")\n",
    "            url = None\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error from API:\", response.json()[\"error\"])\n",
    "        return None\n",
    "\n",
    "def get_user_ids_data(user_ids_list, all_users_filepath):\n",
    "    user_ids_before_user_id = find_user_ids_before_user_id(user_ids_list, all_users_filepath)\n",
    "    user_ids_data = {}\n",
    "    for user_id in user_ids_list:\n",
    "        user_id_before = user_ids_before_user_id[user_id]\n",
    "        if user_id_before:\n",
    "            # 'after' is used to reference that the API returns newest users first\n",
    "            # they are are stored and processed in straightforward, reverse chronilogical order \n",
    "            # it only returns data for the user 'after' a given a user_id in the list (created before timewise)\n",
    "            # so we give it the user_id before it to get the one we want\n",
    "            user_data = get_single_user_data_after(user_id_before)\n",
    "            if user_data:\n",
    "                if 'id' in user_data and user_data['id'] == user_id:\n",
    "                    # Sanity check, should only fail if the all_users dict is not in order of createdTime descending\n",
    "                    user_dict = {}\n",
    "                    user_dict['id'] = user_data\n",
    "                    user_ids_data.update(user_dict)\n",
    "                else:\n",
    "                    print('Error retrieving correct user data for list_users().')\n",
    "            else:\n",
    "                print('Error retrieving user data for user_id: {user_id}')\n",
    "        else:\n",
    "            # this may fail if the user is the last user created, which is unlikely to be used this way\n",
    "            print(f'Error: Could not retreieve data for user_id before: {user_id}')\n",
    "    return user_ids_data\n",
    "    \n",
    "def get_usernames_data(usernames_list, all_users_filepath):\n",
    "    user_ids_before_username = find_user_ids_before_usernames(usernames_list, all_users_filepath)\n",
    "    usernames_data = {}\n",
    "    for username in usernames_list:\n",
    "        user_id_before = user_ids_before_username[username]\n",
    "        if user_id_before:\n",
    "            # 'after' is used to reference that the API returns newest users first\n",
    "            # they are are stored and processed in straightforward, reverse chronilogical order \n",
    "            # it only returns data for the user 'after' a given a user_id in the list (created before timewise)\n",
    "            # so we give it the user_id before it to get the one we want\n",
    "            user_data = get_single_user_data_after(user_id_before)\n",
    "            if user_data:\n",
    "                if 'username' in user_data and user_data['username'] == username:\n",
    "                    # Sanity check, should only fail if the all_users dict is not in order of createdTime descending\n",
    "                    user_dict = {}\n",
    "                    user_dict[username] = user_data\n",
    "                    usernames_data.update(user_dict)\n",
    "                else:\n",
    "                    print('Error retrieving correct user data for list_users().')\n",
    "            else:\n",
    "                print('Error retrieving user data for username: {username}')\n",
    "        else:\n",
    "            # this may fail if the user is the last user created, which is unlikely to be used this way\n",
    "            print(f'Error: Could not retreieve data for username before: {username}')\n",
    "    return usernames_data\n",
    "\n",
    "# restrict include to only top level\n",
    "def print_nested_dict_with_custom_format(data, keys_to_include=None, indent=0):\n",
    "    if keys_to_include is None:\n",
    "        keys_to_include = []\n",
    "\n",
    "    # Define the indentation\n",
    "    spaces = ' ' * indent\n",
    "    \n",
    "    # Loop through the dictionary items\n",
    "    for key, value in sorted(data.items()):\n",
    "        if keys_to_include == [] or key in keys_to_include:\n",
    "            # If the value is a nested dictionary, recursively call the function\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"{spaces}{key:15s}:\")\n",
    "                # only restrict include to top level\n",
    "                print_nested_dict_with_custom_format(value, keys_to_include=None, indent = indent + 4)\n",
    "            else:\n",
    "                # Print the key and its value\n",
    "                if type(value) is float:\n",
    "                    print(f\"{spaces}{key:15s}: {value:16.2f}\")\n",
    "                else:\n",
    "                    print(f\"{spaces}{key:15s}: {value}\")\n",
    "\n",
    "def print_usernames_data_elements(usernames_list, usernames_data, elements_list):\n",
    "    for username in usernames_list:\n",
    "        user_data = usernames_data[username]\n",
    "        print(f\"Username: {username}\")\n",
    "        print_nested_dict_with_custom_format(user_data, elements_list, 2)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05800e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This queries the API to get the latest updated data rather than the cached data\n",
    "\n",
    "usernames_list = [\n",
    "    'MarcusAbramovitch',\n",
    "    'NamesAreHard',\n",
    "    'firstuserhere',\n",
    "    'BTE',\n",
    "    'PunishedFurry',\n",
    "    'PC',\n",
    "    'UFTG',\n",
    "    'Akzzz123',\n",
    "    'AlexbGoode',\n",
    "    'NiallWeaver',\n",
    "    'SemioticRivalry',\n",
    "    'MichaelWheatley',\n",
    "    'chilli',\n",
    "    'optimusprime',\n",
    "    'jack',\n",
    "    'johnleoks',\n",
    "    'Gen',\n",
    "    'hmys',\n",
    "    'Mira',\n",
    "    'levifinkelstein'\n",
    "]\n",
    "\n",
    "'''\n",
    "usernames_list = [\n",
    "    'SirCryptomind'\n",
    "]\n",
    "'''\n",
    "local_timezone = datetime.now().astimezone().tzinfo\n",
    "last_update_time = datetime.now(local_timezone)\n",
    "last_update_time_str = datetime.now(local_timezone).isoformat()\n",
    "# usernames_data = get_usernames_data(usernames_list, all_users_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8436bd1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print using the data just gathered\n",
    "# What properties to print\n",
    "#elements_list = ['balance', 'profitCached', 'totalDeposits']\n",
    "#elements_list = ['profitCached']\n",
    "#print(f\"Current time: {last_update_time_str}\")\n",
    "#print_usernames_data_elements(usernames_list, usernames_data, elements_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc415c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(usernames_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a20f04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_ids = ['b3WDWY8TdrhQKKNuJkNuvQKwHWE3']\n",
    "#print(get_user_ids_data(user_ids, all_users_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b7cadcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_url = \"https://manifold.markets/api/v0/user/by-id/\"\n",
    "\n",
    "# use absolute path so we can use this in other notebooks\n",
    "updates_to_users_filepath = \"/home/db/Documents/JRPdata/manifold-users/updates_to_users.json\"\n",
    "\n",
    "import pytz\n",
    "# timezone used for DAU in manifold (if the measurement is indeed done at midnight) (this is UTC-08:00)\n",
    "timezone_pacific = pytz.timezone('Pacific/Pitcairn')\n",
    "\n",
    "def get_users_in_time_range(min_timestamp, max_timestamp, all_users_filepath):\n",
    "    # Check if max_timestamp is \"Now\" (case-insensitive) and replace it with the current timestamp\n",
    "    if max_timestamp.lower() == \"now\":\n",
    "        local_timezone = datetime.now().astimezone().tzinfo\n",
    "        max_timestamp = datetime.now(local_timezone).isoformat()\n",
    "\n",
    "    try:\n",
    "        # Read all_users from the file\n",
    "        with open(all_users_filepath, \"r\") as file:\n",
    "            all_users = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {all_users_filepath} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON data in {all_users_filepath}.\")\n",
    "        return\n",
    "\n",
    "    # Convert the timestamp strings to datetime objects\n",
    "    min_time = datetime.fromisoformat(min_timestamp)\n",
    "    max_time = datetime.fromisoformat(max_timestamp)\n",
    "\n",
    "    # Count users whose createdTime is within the given time range\n",
    "    count = 0\n",
    "    for user_id, user_data in all_users.items():\n",
    "        if \"createdTime\" in user_data:\n",
    "            created_time = datetime.fromtimestamp(user_data[\"createdTime\"] / 1000, tz=timezone_pacific)\n",
    "            if min_time <= created_time <= max_time:\n",
    "                count += 1\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of users created between {min_timestamp} and {max_timestamp}: {count}\")\n",
    "\n",
    "def get_user_data_by_user_id(user_id):\n",
    "    if user_id is None:\n",
    "        print(\"Error: must supply user id for get_user_data_by_user_id()\")\n",
    "        return None\n",
    "    url = user_url + user_id\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Do not hammer the API\n",
    "        time.sleep(1)\n",
    "        if 'id' in data and user_id == data['id']:\n",
    "            # got the correct data\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Error: no data returned by API in get_user_data_by_user_id() for user id: {user_id}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error from API:\", response.json()[\"error\"])\n",
    "        return None\n",
    "\n",
    "def update_user_data_by_user_ids(user_ids):\n",
    "    users_data = {}\n",
    "    print(f\"User ids: {user_ids}\")\n",
    "    print(\"\")\n",
    "    for user_id in user_ids:\n",
    "        print(f\"Updating for: {user_id}\")\n",
    "        user_data = get_user_data_by_user_id(user_id)\n",
    "        users_data[user_id] = user_data\n",
    "    return users_data\n",
    "\n",
    "# keep updates separate from main file, which is used to track all users\n",
    "# this is for purposes of keeping historical data and tracking other info such as betting frequencies\n",
    "# will store a nested dict: each dict is an update whose key is the update time\n",
    "#  each update dict contains a dict for each user in the update, whose key is the user id\n",
    "def update_users(user_ids, updates_to_users_filepath):\n",
    "    update_time = datetime.now(tz=timezone_pacific).isoformat()\n",
    "    new_update = {}\n",
    "    users_data = update_user_data_by_user_ids(user_ids)\n",
    "    if users_data is None:\n",
    "        print(\"Not updating with update_users(): no users_data\")\n",
    "        return\n",
    "    \n",
    "    new_update[update_time] = users_data\n",
    "    num_users = len(users_data)\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(updates_to_users_filepath):\n",
    "        # Check if updates_to_users is not an empty dict and not None\n",
    "        if users_data is not None and isinstance(users_data, dict) and users_data:\n",
    "            # Write the json to the file at updates_to_users_filepath\n",
    "            with open(updates_to_users_filepath, \"w\") as file:\n",
    "                json.dump(new_update, file)\n",
    "            print(f\"Updated data for # users: {num_users}\")\n",
    "        else:\n",
    "            print(\"Error: list of users retreived from API is empty or None\")\n",
    "    else:\n",
    "        try:\n",
    "            # Read the existing data from the file (assuming it contains an dict)\n",
    "            with open(updates_to_users_filepath, \"r\") as file:\n",
    "                old_updates = json.load(file)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle the case when the file contains invalid JSON data\n",
    "            print(f\"Error: Invalid JSON data in {updates_to_users_filepath}. Not updating.\")\n",
    "            return\n",
    "        # put the old updates after new_updates\n",
    "        new_update.update(old_updates)\n",
    "        with open(updates_to_users_filepath, \"w\") as file:\n",
    "            json.dump(new_update, file)\n",
    "        print(f\"Updated data for # users: {num_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eebfd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user ids in range\n",
    "# get sample of users\n",
    "\n",
    "import random\n",
    "import pytz\n",
    "# timezone used for DAU in manifold (if the measurement is indeed done at midnight) (this is UTC-08:00)\n",
    "timezone_pacific = pytz.timezone('Pacific/Pitcairn')\n",
    "\n",
    "def get_user_ids_created_in_time_range(min_timestamp, max_timestamp, all_users_filepath):\n",
    "    try:\n",
    "        # Read all_users from the file\n",
    "        with open(all_users_filepath, \"r\") as file:\n",
    "            all_users = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {all_users_filepath} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON data in {all_users_filepath}.\")\n",
    "        return\n",
    "\n",
    "    # Convert the timestamp strings to datetime objects\n",
    "    min_time = datetime.fromisoformat(min_timestamp)\n",
    "    max_time = datetime.fromisoformat(max_timestamp)\n",
    "\n",
    "    user_ids = []\n",
    "    # Count users whose createdTime is within the given time range\n",
    "    count = 0\n",
    "    for user_id, user_data in all_users.items():\n",
    "        if \"createdTime\" in user_data:\n",
    "            created_time = datetime.fromtimestamp(user_data[\"createdTime\"] / 1000, tz=timezone_pacific)\n",
    "            if min_time <= created_time <= max_time:\n",
    "                count += 1\n",
    "                user_ids.append(user_data['id'])\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of users created between {min_timestamp} and {max_timestamp}: {count}\")\n",
    "    return user_ids\n",
    "\n",
    "def get_sample_of_user_ids(user_ids_in_time_range, sample_n):\n",
    "    if len(user_ids_in_time_range) <= sample_n:\n",
    "        return user_ids_in_time_range\n",
    "    else:\n",
    "        return random.sample(user_ids_in_time_range, sample_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5138df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_user_data_by_user_id(user_id, all_users_filepath):\n",
    "    try:\n",
    "        # Read all_users from the file\n",
    "        with open(all_users_filepath, \"r\") as file:\n",
    "            all_users = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {all_users_filepath} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON data in {all_users_filepath}.\")\n",
    "        return\n",
    "\n",
    "    return all_users[user_id]\n",
    "\n",
    "def get_cached_users_data_by_user_id(user_ids, all_users_filepath):\n",
    "    try:\n",
    "        # Read all_users from the file\n",
    "        with open(all_users_filepath, \"r\") as file:\n",
    "            all_users = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {all_users_filepath} not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON data in {all_users_filepath}.\")\n",
    "        return\n",
    "\n",
    "    users_data = {}\n",
    "    for user_id in user_ids:\n",
    "        users_data[user_id] = all_users[user_id]\n",
    "    return users_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65423d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e9faa8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users created between 2023-07-25T00:00:00-08:00 and 2023-08-04T00:00:00-08:00: 6005\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "sample_n = 100\n",
    "# sample n users created from the date range:\n",
    "min_timestamp = \"2023-07-25T00:00:00-08:00\"\n",
    "max_timestamp = \"2023-08-04T00:00:00-08:00\"\n",
    "\n",
    "#user_ids_in_time_range = get_user_ids_created_in_time_range(min_timestamp, max_timestamp, all_users_filepath)\n",
    "#sample_user_ids = get_sample_of_user_ids(user_ids_in_time_range, sample_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5206d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
